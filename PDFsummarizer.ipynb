{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohzC711OgKg9",
        "outputId": "c18e79db-b040-476d-c05f-2024d4b4e22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF downloaded successfully and saved as 'mlppt.pdf'\n",
            "Available chapters: ['MACHINE LEARNING', 'APPLICATIONS', 'SUPERVISED LEARNING', 'REGRESSION PROBLEM', 'CLASSIFICATION PROBLEM', 'UNSUPERVISED LEARENING', 'Unsupervised Learning.\\nSupervised Learning\\n\\nx\\nx', 'Q\\n\\nXy\\n\\nX2\\n\\nUnsupervised Learning\\n\\n6', 'O\\n\\nXy\\n\\x0c\\nLINEAR REGRESSION', 'LINEAR REGRESSION', 'MODEL REPRESENTATION\\nMODEL REPRESENTATION\\nMODEL REPRESENTATION', 'MODEL REPRESENTATION\\nGiven a training set ,\\nHypothesis :\\nwhere\\nHow to choose\\nMODEL REPRESENTATION\\nCOST FUNCTION\\nIdea : Choose , so that is as close to y\\nfor our training examples (x , y) .\\nCOST FUNCTION\\nSo going with the idea ,\\nIf hypothesis :\\nWe want to minimise the squared error function\\ni.e. i.e the difference\\nbetween the predicted value ( ) and the\\nactual label (y) should be as minimum as\\npossible.\\nCOST FUNCTION\\nTherefore, the cost function becomes\\nwhere summation( ) represents sum over my\\ntraining set from i to m .\\nand where ( ) represents that minimising one\\nhalf of something shall give us same values of\\nand as minimising the entire thing.\\nCOST FUNCTION\\nSo, overall we want to find the value of &\\nthat minimises the entire expression.\\nGRADIENT DESCENT ALGORITHM', 'GRADIENT DESCENT ALGORITHM', 'GRADIENT DESCENT INTUITION\\nFor eg :) say we have only one parameter, so,\\n& , hence the GD equation becomes\\nis the derivative term which basically\\ndenotes the slope of the line that is just\\ntangent to the function at the point\\nGRADIENT DESCENT INTUITION\\nGRADIENT DESCENT INTUITION\\nGRADIENT DESCENT INTUITION\\nIf is too small , then\\ngradient descent will\\nconverge slowly\\nbecause very small and\\nhence lot of steps are\\nbeing taken before it\\ngets anywhere close to\\nthe global minimum.\\nGRADIENT DESCENT INTUITION\\nIf is too large, then\\ngradient descent will\\novershoot the minimum\\nbecause the steps taken\\nare huge . It may fail to\\nconverge .\\nGRADIENT DESCENT FOR LINEAR\\nREGRESSION\\nSo, applying GD algorithm to minimise the\\nsquared error cost function i.e.\\nwhere ,\\n&\\nCalculating the derivative part,\\nGRADIENT DESCENT FOR LINEAR\\nREGRESSION', 'GRADIENT DESCENT FOR LINEAR\\nREGRESSION\\nNow , by applying GD algorithm to the previous\\nequation(derivative part) ,\\nFEATURE SCALING', 'LINEAR REGRESSION WITH MULTIPLE\\nVARIABLES\\nfor multiple features ,', 'So,\\nPOLYNOMIAL REGRESSION', 'Quadratic model :\\nCubic model :\\nPOLYNOMIAL REGRESSION\\nThe form of hypothesis is :\\nwhere , ,', 'NORMAL EQUATION', 'To minimise the above quadratic function, we\\ncalculate the derivative of the function and set it\\nto 0 i.e. & solve for .\\nNORMAL EQUATION\\nGRADIENT DESCENT Vs NORMAL\\nEQUATION\\nGRADIENT DESCENT NORMAL EQUATION']\n",
            "Hello! I am the Retrieval Learning Bot. Type the name of a chapter to get a summary. Type 'bye' to end.\n",
            "You: MACHINE LEARNING\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 150, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  It is the science of getting computer to learn without being programmed to learn . It is also the science to get computers to learn more quickly and more easily .\n",
            "You: APPICATIONS\n",
            "Bot: Sorry, I couldn't find the chapter you're looking for.\n",
            "You: APPLICATIONS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 150, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot:  • Search Engines like Google, Bing etc. • Facebook photo tagging application. • Self Customizing Programs and many more. • Search engines such as Google and Facebook .\n",
            "You: Bye\n",
            "Bot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber requests pytesseract pdf2image transformers nltk\n",
        "\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from transformers import pipeline\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the summarizer pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Function to download and save the PDF\n",
        "def download_pdf(url, filename):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"PDF downloaded successfully and saved as '{filename}'\")\n",
        "    else:\n",
        "        print(\"Failed to download the PDF.\")\n",
        "\n",
        "# Function to perform OCR on an image and extract text\n",
        "def ocr_image(image):\n",
        "    return pytesseract.image_to_string(image)\n",
        "\n",
        "# Function to extract text from PDF using pdfplumber and OCR where necessary\n",
        "def extract_pdf_text(pdf_path):\n",
        "    text = ''\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages):\n",
        "            page_text = page.extract_text()\n",
        "\n",
        "            # If text is empty, use OCR on page image\n",
        "            if not page_text:\n",
        "                images = convert_from_path(pdf_path, first_page=page_num + 1, last_page=page_num + 1)\n",
        "                for img in images:\n",
        "                    page_text = ocr_image(img)\n",
        "\n",
        "            text += page_text + \"\\n\" if page_text else ''\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to find all likely chapter titles in the PDF\n",
        "def find_chapter_titles(full_text):\n",
        "    titles = re.findall(r'\\n([A-Z][A-Za-z0-9\\s:,.()\\-&]+(?:Chapter|CHAPTER|Section|SECTION)?[A-Za-z0-9\\s]*)\\n', full_text)\n",
        "    return [title.strip() for title in titles]\n",
        "\n",
        "# Function to summarize text using NLP\n",
        "def generate_summary(text):\n",
        "    if len(text) < 50:\n",
        "        return \"Content is too short to summarize.\"\n",
        "    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Function to get chapter text and summarize it\n",
        "def generate_chapter_summary(chapter_title, full_text):\n",
        "    pattern = re.compile(rf\"{re.escape(chapter_title)}(.*?)(?=\\n[A-Za-z0-9\\s:,.()\\-&]+(?:Chapter|CHAPTER|Section|SECTION)?\\n|$)\", re.DOTALL)\n",
        "    match = pattern.search(full_text)\n",
        "\n",
        "    if match:\n",
        "        chapter_content = match.group(1).strip()\n",
        "        return generate_summary(chapter_content)\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find the chapter you're looking for.\"\n",
        "\n",
        "# Main function to start bot and process chapters\n",
        "def start_bot(pdf_path, url):\n",
        "    download_pdf(url, pdf_path)\n",
        "\n",
        "    # Extract text from PDF with OCR assistance\n",
        "    pdf_text = extract_pdf_text(pdf_path)\n",
        "\n",
        "    # Detect and display chapter titles\n",
        "    chapter_titles = find_chapter_titles(pdf_text)\n",
        "    print(\"Available chapters:\", chapter_titles)\n",
        "\n",
        "    # Interactive bot loop\n",
        "    print(\"Hello! I am the Retrieval Learning Bot. Type the name of a chapter to get a summary. Type 'bye' to end.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'bye':\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Generate summary for the requested chapter\n",
        "        summary = generate_chapter_summary(user_input, pdf_text)\n",
        "        print(\"Bot:\", summary)\n",
        "\n",
        "# Specify the PDF URL and path\n",
        "pdf_url = 'https://www.iitp.ac.in/~ai-nlp-ml/resources/talks/mlppt.pdf'\n",
        "pdf_path = 'mlppt.pdf'\n",
        "\n",
        "# Start the bot\n",
        "start_bot(pdf_path, pdf_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Poppler in Google Colab\n",
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRDe1Fm6gMVr",
        "outputId": "29b0ec65-d414-4f8b-a1e0-5ad5a06289e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (396 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXIvz7AmgMFb",
        "outputId": "75721ddf-5a44-4708-a9ce-32401ee8b278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (2,747 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123653 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    }
  ]
}